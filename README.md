# NegaBot: Product-Criticism-Catcher
NegaBot is an AI-powered tool designed to detect negative product reviews on X (formerly Twitter). Trained on thousands of labeled tweets, it accurately classifies whether a tweet expresses dissatisfaction about a product. This can help brands track product criticism in real-time.

## Model Weights
- [NegaBot](https://huggingface.co/jatinmehra/NegaBot-Product-Criticism-Catcher)

## Dataset
- Source: Kaggle

- Total Tweets: 7,920
    - Positive: 5,894
    - Negative: 2,026

- Average Tweet Length:
    - Positive: 118 characters
    - Negative: 96 characters


## Experiments & Training

### Baseline: Traditional ML Models

-   **Preprocessing**:
    -   Removed punctuation, special characters
        
-   **Feature Engineering**:
    -   `TfidfVectorizer` with `ngram_range=(1,6)`
        
-   **Models Evaluated**:
    -   Logistic Regression â†’ Mean F1: **0.78**
    -   Random Forest Classifier â†’ Mean F1: **0.78**
    -   XGBoost â†’ Mean F1: **0.79**
    -   Support Vector Classifier â†’ Mean F1: **0.81**
        
-   **Test Set Accuracy**: **0.80**
-   **Validation**:
    -   `StratifiedKFold` with `k=5`
-   **Tools Used**:
    -   `scikit-learn`, `xgboost`, `nltk`
        

----------

### Baseline LLM: BERT Fine-tuning

-   **Model**: `bert-base-uncased` from Hugging Face Transformers
-   **Preprocessing**:
    -   Removed emojis, HTML tags, punctuation, and special characters
-   **Dataset Handling**:
    -   Converted `pandas` DataFrame to Hugging Face `Dataset`
-   **Tokenizer**:
    -   AutoTokenizer (BERT-based)
-   **Training**:
    -   Epochs: **3**
    -   Learning rate: **5e-5**
    -   Batch size: **16**
    -   Max sequence length: **256**
    -   Evaluation every 200 steps
-   **Results**:
    -   Validation Accuracy: **~89%**
    -   Test Accuracy: **0.84**
-   **Tools Used**:
    -   `transformers`, `datasets`, `torch`, `sklearn`
        

----------

### Bigger Model: SmolLM 360M

-   **Model**: `SmolLM 360M` (360M parameter model from Hugging Face)
-   **Preprocessing**:
    -   Same as BERT (later tested with no-cleaning)
    -   Identical to BERT setup
-   **Results**:
    -   Validation Accuracy: **0.90**
    -   Test Accuracy: **0.86**
-   **Key Insight**:
    -   Removing cleaning entirely improved accuracy â†’ Final Test Accuracy: **0.91**
-   **Observation**:
    -   The model likely learned from emoji/special character patterns in raw data.

## Model Architecture Choices

### BERT (Baseline LLM)
-   **Type**: Encoder-only Transformer
-   **Why BERT?**
    -   Ideal for classification tasks due to bidirectional context understanding
    -   Well-suited for tweet-sized inputs and short text sentiment tasks
    -   Fast and reliable fine-tuning process

### SmolLM 360M
-   **Type**: Decoder-only Transformer (Causal Language Model)
-   **Why SmolLM?**
    -   Capable of generalization on noisy social media data
    -   Outperformed BERT when trained on uncleaned data
    -   Larger capacity helps capture subtle sentiment patterns (e.g., sarcasm, emojis)

### ðŸ¤” Other Models Considered

-   **RoBERTa**: More robust than BERT but slower to train; not chosen for baseline.
-   **DistilBERT**: Lighter and faster but underperformed in early experiments.
-   **SmolLM** provided an opportunity to test encoder vs decoder model styles on sentiment classification.

### ðŸ“Š Performance Summary

| Model          | Type             | Parameters | Validation Acc | Test Acc | Notes                |
|----------------|------------------|------------|----------------|----------|----------------------|
| BERT-base      | Encoder-only     | ~110M      | ~0.89          | 0.84     | Cleaned tweets       |
| SmolLM 360M    | Decoder-only     | ~360M      | 0.90           | **0.86** | Cleaned tweets 		|
| SmolLM 360M    | Decoder-only     | ~360M      | 0.90           | **0.91** | Raw (uncleaned) tweets |